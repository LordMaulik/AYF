{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install --force-reinstall pydantic==1.10.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MK_nEmc_Czbm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import transformers as tr\n",
        "from datasets import load_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7svihdONEa8I"
      },
      "outputs": [],
      "source": [
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"########################\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTX5oaRbEeRU"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_column', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_seq_items', None)\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "pd.set_option('expand_frame_repr', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-ViD6JeEkCS"
      },
      "outputs": [],
      "source": [
        "import tempfile\n",
        "tmpdir = tempfile.TemporaryDirectory()\n",
        "local_training_root = tmpdir.name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3wRKfu4Elxz"
      },
      "outputs": [],
      "source": [
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"9994\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Jz5G80cEnWx"
      },
      "outputs": [],
      "source": [
        "imdb_ds = load_dataset(\"data.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyCdJrB6EoFp"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/gemma-7b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y3fFCIAEpZr"
      },
      "outputs": [],
      "source": [
        "tokenizer = tr.AutoTokenizer.from_pretrained(model_name, cache_dir=tmpdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ9o3oc7DbpQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def to_tokens(tokenizer, label_map):\n",
        "    def apply(x):\n",
        "        target_labels = [label_map[y] for y in x[\"label\"]]\n",
        "        token_res = tokenizer(\n",
        "            x[\"text\"],\n",
        "            text_target=target_labels,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "        )\n",
        "        # Convert tensors to lists or numpy arrays\n",
        "        for key, value in token_res.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                token_res[key] = value.tolist()\n",
        "        return token_res\n",
        "    return apply\n",
        "\n",
        "# Create function to convert IMDb dataset to tokens\n",
        "imdb_to_tokens = to_tokens(tokenizer=imdb_ds)\n",
        "\n",
        "# Tokenize the IMDb dataset\n",
        "tokenized_dataset = imdb_ds.map(\n",
        "    imdb_to_tokens,\n",
        "    batched=True,  # Expect the function to return a dictionary of types like (<class 'list'>, <class 'numpy.ndarray'>).\n",
        ")\n",
        "\n",
        "def test_tokenized_dataset(tokenized_dataset, num_samples=1):\n",
        "    # Print the first few samples from the tokenized dataset\n",
        "    for i in range(num_samples):\n",
        "        sample = tokenized_dataset[i]\n",
        "        print(f\"Sample {i + 1}:\")\n",
        "        print(\"Input IDs:\", sample[\"ids\"])\n",
        "        print(\"Labels:\", sample[\"labels\"])\n",
        "        print(\"=\" * 10)\n",
        "\n",
        "# Test the tokenized dataset\n",
        "test_tokenized_dataset(tokenized_dataset['labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdaaM_QrDiF0"
      },
      "outputs": [],
      "source": [
        "# Define Zero configuration for optimization\n",
        "zero_config = {\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},\n",
        "        \"allgather_partitions\": True,\n",
        "        \"allgather_bucket_size\": 5e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"reduce_scatter\": True,\n",
        "        \"reduce_bucket_size\": 5e8,\n",
        "        \"contiguous_gradients\": True,\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\",\n",
        "            \"torch_adam\": True,\n",
        "        },\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\"\n",
        "}\n",
        "\n",
        "# Specify the model checkpoint to use\n",
        "model_checkpoint = \"base\"\n",
        "\n",
        "tokenizer = tr.AutoTokenizer.from_pretrained(\n",
        "    model_checkpoint, cache_dir=tempfile\n",
        ")\n",
        "\n",
        "# Load the model for sequence-to-sequence learning\n",
        "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_checkpoint, cache_dir=tempfile\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFiIkC48DkFx"
      },
      "outputs": [],
      "source": [
        "# Define checkpoint name and location\n",
        "checkpoint_name = \"test\"\n",
        "checkpoint_location = os.path.join(local_training_root, checkpoint_name)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = tr.TrainingArguments(\n",
        "    checkpoint_location,\n",
        "    num_train_epochs=120,\n",
        "    per_device_train_batch_size=16,\n",
        "    deepspeed=zero_config,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTLfMmB8F2jD"
      },
      "outputs": [],
      "source": [
        "# Define data collator\n",
        "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Create compute instance\n",
        "compute = tr.compute(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUBQ4op5GzEQ"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "compute.train()\n",
        "\n",
        "# Persist the fine-tuned model to a directory\n",
        "final_model_path = f\"{tempfile}/{checkpoint_name}\"\n",
        "compute.save_model(output_dir=final_model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tra6zjdeFzjI"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = tr.AutoModelForSeq2SeqLM.from_pretrained(final_model_path)\n",
        "\n",
        "# Generate predictions\n",
        "inputs = tokenizer(\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=True,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "prediction = fine_tuned_model.generate(\n",
        "    system_prompt=\"Analyze the Sentimental analysis and score it. Also, perfrom the Writing Style Evaluation.\"\n",
        "    input_ids=inputs[\"input_ids\"],\n",
        "    attention_mask=inputs[\"attention_mask\"]\n",
        ")\n",
        "\n",
        "print(prediction)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
